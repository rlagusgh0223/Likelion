{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "211008(텐서플로우 1.x)",
      "provenance": [],
      "authorship_tag": "ABX9TyOfb3RgtpKBXh89k+Bp8Tuo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1d2k-C-TsBk",
        "outputId": "7d369a57-e859-42dc-a995-b4672f1cf20a"
      },
      "source": [
        "## 여러 입력 값을 갖는 로지스틱 회귀 실제 값 적용하기\n",
        "#7시간 공부하고 과외를 6번 받은 학생의 합격 가능성\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "#실행할 때마다 같은 결과를 출력하기 위한 seed값 설정\n",
        "seed = 0\n",
        "np.random.seed(seed)    #넘파이, 텐서플로우 seed값 초기화\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "#x, y의 데이터 값\n",
        "x_data = np.array([[2,3],[4,3],[6,4],[8,6],[10,7],[12,8],[14,19]])    #[공부시간, 과외횟수]\n",
        "y_data = np.array([0,0,0,1,1,1,1]).reshape(7,1)    #가로 7, 세로1로 만든다. 합격/불합격 결과\n",
        "\n",
        "#입력 값을 플레이스 홀더에 저장\n",
        "X = tf.placeholder(tf.float64, shape=[None, 2])    #앞은 상관없고, 1차원 요소는 2개\n",
        "Y = tf.placeholder(tf.float64, shape=[None, 1])    #앞은 상관없고, 1차원 요소는 1개\n",
        "\n",
        "#기울기 a와 바이어스 b의 값을 임의로 정함\n",
        "a = tf.Variable(tf.random_uniform([2,1], dtype=tf.float64))\n",
        "#기울기 2개니까 a1, a2 묶어서 [2,1](하나로 표현된 식이 2개 있다)\n",
        "#a는 [2,1] 형태를 가짐 a1, a2\n",
        "#[2,1] 의미 : 들어오는 값은 2개, 나가는 값은 1개\n",
        "b = tf.Variable(tf.random_uniform([1], dtype=tf.float64))\n",
        "\n",
        "#y 시그모이드 함수의 방정식을 세움\n",
        "y = tf.sigmoid(tf.matmul(X, a) + b)    #예측치. () 안 부분은 -z식\n",
        "\n",
        "#오차를 구하는 함수\n",
        "loss = -tf.reduce_mean(Y * tf.log(y) + (1 - Y) * tf.log(1 - y))\n",
        "\n",
        "#학습률 값\n",
        "learning_rate = 0.1\n",
        "\n",
        "#오차를 최소로 하는 값 찾기\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "predicted = tf.cast(y > 0.5, dtype=tf.float64) \n",
        "#y가 0.5보다 크면 1, 아니면 0(텐서플로우의 cast연산자에 의해)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
        "#텐서플로우의 평균(predicted와 Y가 같으면 1, 아니면 0인 값들의 평균)\n",
        "\n",
        "#학습\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(3001):\n",
        "        a_, b_, loss_, _ = sess.run([a, b, loss, gradient_decent], feed_dict={X:x_data, Y:y_data})\n",
        "        #플레이스홀더가 있으면 feed_dict로 값을 넣어준다\n",
        "        if (i+1) % 300 == 0:\n",
        "            print(\"step = %d, a1 = %.4f, a2 = %.4f, b = %.4f, loss = %.4f\"\n",
        "            %(i+1, a_[0], a_[1], b_, loss_))\n",
        "    \n",
        "    \n",
        "    print()\n",
        "    #추가 코드(sessoin안에 있어야 함)\n",
        "    print(\"predicted = \", sess.run(predicted, feed_dict={X:x_data})) #\n",
        "    #다른 값 테스트\n",
        "    p_val, h_val = sess.run([predicted, y], feed_dict={X:[[1, 5], [10, 5], [4, 5]]})\n",
        "    print(\"check predicted = \", p_val)    #연산된 값을 bool로 변환한 값\n",
        "    print(\"check hypothesis = \", h_val)   #계산값\n",
        "    #정확도 측정\n",
        "    h, c, a = sess.run([y, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "    print(\"\\nHypothesis : \", h, \"\\nCorrect (Y) :\", c, \"\\nAccuracy : \",a)    #예측치, 결과, 정확도 인듯....\n",
        "    \n",
        "    #===========================================여기부터 오늘 코드=============================================\n",
        "    new_x = np.array([7,6]).reshape(1,2)    #7,6은 각각 공부한 시간과 과외수업횟수\n",
        "    new_y, new_y_result = sess.run([y, predicted], feed_dict={X:new_x})\n",
        "    #새 합격 가능성, 그걸 predicted한게 y_result, 데이터를 플레이스 홀더에 넣어준다\n",
        "    \n",
        "    print(\"공부한 시간 : %d, 과외 수업 횟수 : %d\" %(new_x[:,0], new_x[:,1]))\n",
        "    print(\"합격 가능성 : %6.2f %%\" %(new_y*100))    #%하나만 쓰면 %글자 출력 안되니까 %%쓴거다\n",
        "    print(\"예측 결과 :\",new_y_result[0])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "step = 300, a1 = 0.4065, a2 = 0.0859, b = -2.8088, loss = 0.2459\n",
            "step = 600, a1 = 0.5048, a2 = 0.2208, b = -4.2800, loss = 0.1711\n",
            "step = 900, a1 = 0.4974, a2 = 0.4251, b = -5.2946, loss = 0.1351\n",
            "step = 1200, a1 = 0.4489, a2 = 0.6468, b = -6.0918, loss = 0.1121\n",
            "step = 1500, a1 = 0.3886, a2 = 0.8598, b = -6.7544, loss = 0.0958\n",
            "step = 1800, a1 = 0.3273, a2 = 1.0563, b = -7.3229, loss = 0.0836\n",
            "step = 2100, a1 = 0.2689, a2 = 1.2353, b = -7.8217, loss = 0.0741\n",
            "step = 2400, a1 = 0.2148, a2 = 1.3980, b = -8.2662, loss = 0.0665\n",
            "step = 2700, a1 = 0.1651, a2 = 1.5461, b = -8.6673, loss = 0.0603\n",
            "step = 3000, a1 = 0.1197, a2 = 1.6814, b = -9.0329, loss = 0.0552\n",
            "\n",
            "predicted =  [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "check predicted =  [[0.]\n",
            " [1.]\n",
            " [0.]]\n",
            "check hypothesis =  [[0.37624807]\n",
            " [0.63890395]\n",
            " [0.46336593]]\n",
            "\n",
            "Hypothesis :  [[0.02298783]\n",
            " [0.02901783]\n",
            " [0.16945959]\n",
            " [0.88218362]\n",
            " [0.98081412]\n",
            " [0.99714306]\n",
            " [1.        ]] \n",
            "Correct (Y) : [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "Accuracy :  1.0\n",
            "공부한 시간 : 7, 과외 수업 횟수 : 6\n",
            "합격 가능성 :  86.92 %\n",
            "예측 결과 : [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcNnmuJ_Ui8p",
        "outputId": "eadee634-1198-4705-b9fa-0af8b3f5bb03"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "#배열 슬라이싱 응용\n",
        "xy = np.loadtxt(\"/content/data-03-diabetes.csv\", delimiter=\",\")\n",
        "x_data = xy[:, 0:-1]    #[전체줄, 각 줄 데이터 중 0~맨 끝에거 앞에까지]\n",
        "y_data = xy[:, [-1]]    #[전체줄, 각 줄 데이터 중 맨 끝에거]\n",
        "\n",
        "#Placeholders : Shape주의! 총 8개의 x_data와 1개의 y_data\n",
        "X = tf.placeholder(tf.float32, shape=[None,8])#\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1])#\n",
        "\n",
        "W = tf.Variable(tf.random_normal([8,1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "#random_normal은 어느정도 예측할 수 있을때 그 값을 중심으로 하지만\n",
        "#대부분은 예측할 수 없으니까 uniform을 쓰는게 좋다\n",
        "\n",
        "#Hypothesis예측값\n",
        "hyp = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "#Cost/Loss function\n",
        "cost = -tf.reduce_mean(Y * tf.log(hyp) + (1 - Y) * tf.log(1-hyp))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "#정확도 hyp > 0.5 else False\n",
        "predicted = tf.cast(hyp > 0.5, dtype=tf.float32)#\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))#\n",
        "\n",
        "#세션 시작\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        cost_val, _ = sess.run([cost, train], feed_dict={X:x_data, Y:y_data})\n",
        "        if step % 200 == 0:\n",
        "            print(step, cost_val)\n",
        "            # 10000 0.480384\n",
        "        \n",
        "    #정확도 77%\n",
        "    _, _, a = sess.run([hyp, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "    #hyp, predicted은 연산을 위해 필요한거니까 출력 안해도 됨\n",
        "    #a = sess.run(accuracy, feed_dict={X:x_data, Y:y_data})로 해도 문제는 안됨\n",
        "    #graph가 구성되어 있으므로\n",
        "    print(\"Accuracy :\", a)\n",
        "    #Accuracy : 0.769433"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.0982647\n",
            "200 0.7647606\n",
            "400 0.6994914\n",
            "600 0.67361456\n",
            "800 0.655156\n",
            "1000 0.6394628\n",
            "1200 0.6256324\n",
            "1400 0.61334455\n",
            "1600 0.60239047\n",
            "1800 0.5925983\n",
            "2000 0.58382064\n",
            "2200 0.57592976\n",
            "2400 0.56881547\n",
            "2600 0.562383\n",
            "2800 0.55655044\n",
            "3000 0.5512471\n",
            "3200 0.546412\n",
            "3400 0.54199237\n",
            "3600 0.5379424\n",
            "3800 0.5342225\n",
            "4000 0.53079784\n",
            "4200 0.52763826\n",
            "4400 0.52471733\n",
            "4600 0.5220114\n",
            "4800 0.5195003\n",
            "5000 0.51716566\n",
            "5200 0.5149916\n",
            "5400 0.5129636\n",
            "5600 0.5110692\n",
            "5800 0.5092968\n",
            "6000 0.50763637\n",
            "6200 0.5060787\n",
            "6400 0.5046156\n",
            "6600 0.50323975\n",
            "6800 0.5019443\n",
            "7000 0.50072324\n",
            "7200 0.49957117\n",
            "7400 0.498483\n",
            "7600 0.49745417\n",
            "7800 0.49648055\n",
            "8000 0.49555835\n",
            "8200 0.49468398\n",
            "8400 0.49385443\n",
            "8600 0.49306655\n",
            "8800 0.49231783\n",
            "9000 0.49160567\n",
            "9200 0.49092776\n",
            "9400 0.49028206\n",
            "9600 0.48966655\n",
            "9800 0.4890795\n",
            "10000 0.48851916\n",
            "Accuracy : 0.76811594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A8m4DiSVNfv",
        "outputId": "f0ea39e3-23d9-4c54-d181-9ad2391362dd"
      },
      "source": [
        "# 퍼셉트론\n",
        "#AND, OR, NAND를 구현하는 로직연산\n",
        "#퍼셉트론은 XOR못한다\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "#y_data = np.array([[0],[0],[0],[1]], dtype=np.float32)    #And를 구현하는 로직\n",
        "#y_data = np.array([[0],[1],[1],[1]], dtype=np.float32)    #OR를 구현하는 로직\n",
        "y_data = np.array([[1],[1],[1],[0]], dtype=np.float32)     #NAND를 구현하는 로직\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,2])\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hyp = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hyp) + (1 - Y) * tf.log(1-hyp))   #오차\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hyp > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
        "        if step % 1000 == 0:\n",
        "            print(\"step =\", step, \"cost =\", sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
        "            print('W =',sess.run(W), \"b =\",sess.run(b))\n",
        "\n",
        "    h, p, a = sess.run([hyp, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "    print(\"\\nHypothesis :\",h,\"\\nCorrect\",p,\"\\nAccuracy :\", a)\n",
        "    \n",
        "    #실제값 확인\n",
        "    print(\"\\n다른값 테스트\")\n",
        "    new_x = np.array([0,1]).reshape(1,2)\n",
        "    new_y = sess.run(hyp, feed_dict={X:new_x})\n",
        "    h, c = sess.run([hyp, predicted], feed_dict={X:new_x, Y:new_y})\n",
        "    print('Hyothesis :', h, \"\\nCorrect(Y) :\",c)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 0 cost = 0.8677834\n",
            "W = [[1.5349876 ]\n",
            " [0.44916767]] b = [0.31414393]\n",
            "step = 1000 cost = 0.5466203\n",
            "W = [[ 0.17718239]\n",
            " [-0.47129968]] b = [0.8692014]\n",
            "step = 2000 cost = 0.40897816\n",
            "W = [[-0.5855367]\n",
            " [-0.9635568]] b = [1.6038785]\n",
            "step = 3000 cost = 0.33058867\n",
            "W = [[-1.116324 ]\n",
            " [-1.3425575]] b = [2.1993902]\n",
            "step = 4000 cost = 0.2795989\n",
            "W = [[-1.5207171]\n",
            " [-1.6602919]] b = [2.6932638]\n",
            "step = 5000 cost = 0.24325544\n",
            "W = [[-1.8479936]\n",
            " [-1.9366994]] b = [3.116632]\n",
            "step = 6000 cost = 0.21572495\n",
            "W = [[-2.1243367]\n",
            " [-2.1822994]] b = [3.4884691]\n",
            "step = 7000 cost = 0.19398177\n",
            "W = [[-2.3647373]\n",
            " [-2.4035912]] b = [3.8208263]\n",
            "step = 8000 cost = 0.1762886\n",
            "W = [[-2.578371]\n",
            " [-2.605028]] b = [4.1217985]\n",
            "step = 9000 cost = 0.16156566\n",
            "W = [[-2.7711957]\n",
            " [-2.789877 ]] b = [4.397102]\n",
            "step = 10000 cost = 0.1490995\n",
            "W = [[-2.9472878]\n",
            " [-2.9606347]] b = [4.65094]\n",
            "\n",
            "Hypothesis : [[0.99053776]\n",
            " [0.8442643 ]\n",
            " [0.8460111 ]\n",
            " [0.22149372]] \n",
            "Correct [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy : 1.0\n",
            "\n",
            "다른값 테스트\n",
            "Hyothesis : [[0.8442643]] \n",
            "Correct(Y) : [[1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Jc0YY8VWuu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}