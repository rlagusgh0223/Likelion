#텍스트의 토큰화
#주어진 문장을 '단어'로 토큰화 하기

#케라스의 텍스트 전처리와 관련한 함수 중 text_to_word_sequence함수를 불러옵니다.
from tensorflow.keras.preprocessing.text import text_to_word_sequence

#전처리할 텍스트를 정합니다.
text = '해보지 않으면 해낼 수 없다'

#해당 텍스트를 토큰화 합니다.
result = text_to_word_sequence(text)
print("\n원문 :\n", text)
print("\n토큰화 :\n", result)





#텍스트의 토큰화 - 주어진 문장을 '단어'로 토큰화 하기
import numpy
import tensorflow as tf
from numpy import array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding

#단어 빈도수 세기
#전처리 하려는 세개의 문장을 정합니다
docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',
       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',
       '토큰화 한 결과는 딥러닝에서 사용 할 수 있습니다.']

#토큰화 함수를 이용해 전처리 하는 과정입니다.
token = Tokenizer()    #토큰화 함수 지정
token.fit_on_texts(docs)    #토큰화 함수에 문장 적용

#단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다
print("\n단어 카운트 :\n", token.word_counts)
#Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict클래스를 사용합니다.

#출력되는 순서는 랜덤입니다
print("\n문장 카운트 :",token.document_count)
print("\n각 단어가 몇개의 문장에 포함되어 있는가 :\n", token.word_docs)
print("\n각 단어에 매겨진 인덱스 값 :\n", token.word_index)






#단어의 원-핫 인코딩
from tensorflow.keras.preprocessing.text import Tokenizer

text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'

#토큰화
token = Tokenizer()
token.fit_on_texts([text])
print(token.word_index)

x = token.texts_to_sequences([text])
print("\n텍스트, 토큰화 결과 :\n", x)


from tensorflow.keras.utils import to_categorical

#인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기
word_size = len(token.word_index) + 1
x = to_categorical(x, num_classes = word_size)

print(x)







#텍스트를 읽고 긍정, 부정 예측하기
from numpy import array

#텍스트 리뷰 자료를 지정합니다
docs = ["너무 재밌네요", "최고예요", "참 잘 만든 영화예요", "추천하고 싶은 영화입니다",
       "한번 더 보고싶네요", "글쎄요", "별로예요", "생각보다 지루하네요", "연기가 어색해요", "재미없어요"]

#긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다
classes = array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])

from tensorflow.keras.preprocessing.text import Tokenizer
#토큰화
token = Tokenizer()
token.fit_on_texts(docs)
print(token.word_index)
x = token.texts_to_sequences(docs)
print("\n리뷰 텍스트, 토큰화 결과 :\n", x)

from tensorflow.keras.preprocessing.sequence import pad_sequences
#패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다
padded_x = pad_sequences(x, 4)
print("\n패딩 결과 :\n", padded_x)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding

#딥러닝 모델
print("\n딥러닝 모델 시작 :")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding

#딥러닝 모델
print("\n딥러닝 모델 시작 :")

#임베딩에 입력될 단어의 수를 지정합니다.
word_size = len(token.word_index) + 1

#단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다
model = Sequential()
model.add(Embedding(word_size, 8, input_length=4))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(padded_x, classes, epochs=20)
print("\n Accuracy : %.4f" % (model.evaluate(padded_x, classes)[1]))






#단어와 문자의 원-핫 인코딩
import numpy as np

samples = ['The cat sat on th mat.', 'The dog ate my homework.']

#데이터에 있는 모든 토큰의 인덱스 딕셔너리를 구축합니다
token_index = {}    #빈 딕셔너리 생성
for sample in samples:    #sample은 samples의 두개 문장 중 각각의 하나의 문장
    print(sample)
    #split()메서드를 사용해 샘플을 토큰으로 나눕니다
    #split() 에서는 구둣점과 특수 문자도 사용합니다
    #'mat.' 'homework.'과 같이 '.'을 포함하는 문자를 위해 replace('.', ' ')를 사용
    for word in sample.replace('.', ' ').split():    #word 는 key
        if word not in token_index:    #index가 할당되어 있으면 중복 할당 하지 않음
            #단어마다 고유한 인덱스를 할당합니다. len(token_index)는 증가하는 값
            token_index[word] = len(token_index) + 1
            #인덱스 0은 사용하지 않습니다.
print(token_index)

#샘플을 벡터로 변환합니다
#각 샘플에서 max_length까지 단어만 사용합니다
max_length = 10
#결과를 저장할 배열입니다

results = np.zeros((len(samples), max_length, max(token_index.values()) +1))
for i, sample in enumerate(samples):
    print(sample)
    for j, word in list(enumerate(sample.replace('.', ' ').split()))[:max_length]:
        #token_index(위에 만들어진 토큰의 인덱스 딕셔너리)
        index = token_index.get(word)
        results[i, j, index] = 1.
print(results)