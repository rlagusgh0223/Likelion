glove_dir = 'C:/Users/Owner/Desktop/khh/Likelion/211029/glove.6B'

embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')
for line in f:
    values = line.split()
    word = values[0]    #단어
    #1번 인덱스 이후 마지막까지 100차원의 임베딩 벡터 정보
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print("%s개의 단어 벡터를 찾았습니다." %len(embeddings_index))

embedding_dim = 100
#max_words는 사용하는 10000개의 단어
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    #embeddings_index는 문자열과 100개의 벡터가 연결된 딕셔너리
    embedding_vector = embeddings_index.get(word)
    if i < max_words:
        if embedding_vector is not None:
            #임베딩 인덱스에 없는 단어는 모두 0이 됩니다. 위 0 초기화 유지
            embedding_matrix[i] = embedding_vector

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data = (x_val,y_val))
model.save_weights('pre_trained_glove_model.h5')

import matplotlib.pyplot as plt
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()