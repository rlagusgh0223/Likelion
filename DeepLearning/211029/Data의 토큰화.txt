from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
maxlen = 100    #100개 단어 이후는 버립니다
training_samples = 200    #훈련 샘플은 200개입니다
validation_samples = 10000    #검증 샘플은 10,000개입니다
max_words = 10000    #데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)    #단어 인덱스를 구축

#앞서 만들어진 토큰의 인덱스로만 채워진 새로운 배열을 만들어 줌
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index    #문장 전체의 딕셔너리
print("%s개의 고유한 토큰을 찾았습니다." %len(word_index))

#패딩(padding)과정 : 길이를 똑같이 맞춰 주는 작업
data = pad_sequences(sequences, maxlen=maxlen)

labels = np.asarray(labels)
print('데이터 텐서의 크기 :', data.shape)
print('레이블 텐서의 크기 :', labels.shape)

#데이터를 훈련 세트와 검증 세트로 분할합니다
#샘플이 순서대로 있기 때문에(부정 샘플이 모두 나온 후에 긍정 샘플이 옵니다)
#먼저 데이터를 섞습니다.

#data의 전체 수량만큼의 배열을 indices에 생성(할당)합니다
indices = np.arange(data.shape[0])
np.random.shuffle(indices)     #긍정 부정을 섞음(25000개의 index번호를 섞는다는 의미)
data = data[indices]    #data가 섞임, 같은 순서(indices)로
labels = labels[indices]    #labels가 섞임, 같은 순서(indices)로

x_train = data[:training_samples]
y_train = labels[:training_samples]
x_val = data[training_samples : training_samples + validation_samples]
y_val = labels[training_samples : training_samples + validation_samples]