{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "211012(텐서플로우 1.x)",
      "provenance": [],
      "authorship_tag": "ABX9TyMh5sHOi9CteDRjjCHwlCRo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPutkn2dv5Ku",
        "outputId": "b1662aa4-27ed-4eb5-eec3-29c3e6bb0e39"
      },
      "source": [
        "#Hidden Layer 적용하여 XOR 해결\n",
        "#이번주에 할 코드의 기본\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.1\n",
        "tf.set_random_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "x_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_data = [[0], [1], [1], [0]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "#레이어 시작\n",
        "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')    #입력, 출력 2개\n",
        "b1 = tf.Variable(tf.random_normal([2]), name='bias1')    #거기에 대한 바이어스도 2개\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)    #W1, b1를 이용하여 출력 2개가 나오도록 하는 레이어\n",
        "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')    #입력 2개, 출력 1개\n",
        "b2 = tf.Variable(tf.random_normal([1]), name='bias2')    #바이어스 1개\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)    #W2, b2를 이용하여 출력 1개가 나오도록 하는 레이어\n",
        "#여기까지가 그래프를 그리는 레이어다\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)#경사하강법\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
        "        if step % 500 == 0:\n",
        "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W2))\n",
        "            \n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.7315512 [[-1.011666 ]\n",
            " [ 0.6350718]]\n",
            "500 0.66300535 [[-1.1473188]\n",
            " [ 1.1247213]]\n",
            "1000 0.56061697 [[-2.0536354]\n",
            " [ 2.0469878]]\n",
            "1500 0.32521626 [[-3.5266666]\n",
            " [ 3.792276 ]]\n",
            "2000 0.17022364 [[-4.8581185]\n",
            " [ 5.315607 ]]\n",
            "2500 0.10421395 [[-5.801907 ]\n",
            " [ 6.3321996]]\n",
            "3000 0.07250708 [[-6.4866633]\n",
            " [ 7.0461955]]\n",
            "3500 0.054764487 [[-7.012731 ]\n",
            " [ 7.5854626]]\n",
            "4000 0.04366482 [[-7.436047]\n",
            " [ 8.015194]]\n",
            "4500 0.036148988 [[-7.788624]\n",
            " [ 8.370952]]\n",
            "5000 0.030757422 [[-8.08996 ]\n",
            " [ 8.673785]]\n",
            "5500 0.026717525 [[-8.352656]\n",
            " [ 8.93704 ]]\n",
            "6000 0.023586107 [[-8.585255]\n",
            " [ 9.169668]]\n",
            "6500 0.021092607 [[-8.793803]\n",
            " [ 9.37792 ]]\n",
            "7000 0.019062959 [[-8.982712]\n",
            " [ 9.566344]]\n",
            "7500 0.017380564 [[-9.155304]\n",
            " [ 9.73834 ]]\n",
            "8000 0.015964437 [[-9.314123]\n",
            " [ 9.896498]]\n",
            "8500 0.014756865 [[-9.461177]\n",
            " [10.042854]]\n",
            "9000 0.013715377 [[-9.598066 ]\n",
            " [10.1790285]]\n",
            "9500 0.012808429 [[-9.726086]\n",
            " [10.306336]]\n",
            "10000 0.012011716 [[-9.846307]\n",
            " [10.425842]]\n",
            "\n",
            "Hypothesis: \n",
            " [[0.01166028]\n",
            " [0.9844632 ]\n",
            " [0.98930585]\n",
            " [0.00985867]] \n",
            "Correct:\n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:\n",
            " 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkDwomLmw1vQ",
        "outputId": "85b50052-5560-4e51-a61c-804511e14e03"
      },
      "source": [
        "#위의 코드에서 노드 2개를 5개로 바꾼것\n",
        "#정밀도가 늘어남\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.1\n",
        "tf.set_random_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "x_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_data = [[0], [1], [1], [0]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "#레이어 시작\n",
        "#위의 코드보다 노드가 늘어나 정밀도가 높아졌다\n",
        "W1 = tf.Variable(tf.random_normal([2, 5]), name='weight1')    #2->5\n",
        "b1 = tf.Variable(tf.random_normal([5]), name='bias1')    #2->5\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "W2 = tf.Variable(tf.random_normal([5, 1]), name='weight2')    #2->5\n",
        "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "#여기까지가 그래프를 그리는 레이어다\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
        "        if step % 500 == 0:\n",
        "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W2))\n",
        "            \n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.7107358 [[ 1.0141189 ]\n",
            " [ 0.14687352]\n",
            " [-0.39922827]\n",
            " [-0.6600192 ]\n",
            " [-1.2039195 ]]\n",
            "500 0.6710736 [[ 0.90017956]\n",
            " [ 0.08459909]\n",
            " [-0.49944744]\n",
            " [-0.8319839 ]\n",
            " [-1.5542929 ]]\n",
            "1000 0.60293823 [[ 1.1010678 ]\n",
            " [ 0.12409084]\n",
            " [-0.389468  ]\n",
            " [-0.9928593 ]\n",
            " [-2.5272856 ]]\n",
            "1500 0.4848765 [[ 1.7760755 ]\n",
            " [ 0.16069998]\n",
            " [-0.06264772]\n",
            " [-1.3638319 ]\n",
            " [-3.8867874 ]]\n",
            "2000 0.33722085 [[ 2.935657  ]\n",
            " [ 0.23893684]\n",
            " [ 0.28278115]\n",
            " [-1.9486325 ]\n",
            " [-5.243668  ]]\n",
            "2500 0.20530204 [[ 4.1981997]\n",
            " [ 0.408971 ]\n",
            " [ 0.4710893]\n",
            " [-2.498897 ]\n",
            " [-6.4339037]]\n",
            "3000 0.12541045 [[ 5.231383  ]\n",
            " [ 0.6262167 ]\n",
            " [ 0.54609954]\n",
            " [-2.896868  ]\n",
            " [-7.3378816 ]]\n",
            "3500 0.083014324 [[ 6.001606  ]\n",
            " [ 0.8324172 ]\n",
            " [ 0.57770246]\n",
            " [-3.1872108 ]\n",
            " [-7.9925547 ]]\n",
            "4000 0.059493147 [[ 6.580947  ]\n",
            " [ 1.0128691 ]\n",
            " [ 0.59338206]\n",
            " [-3.4121792 ]\n",
            " [-8.480573  ]]\n",
            "4500 0.045311898 [[ 7.031803 ]\n",
            " [ 1.1696688]\n",
            " [ 0.6021838]\n",
            " [-3.5946171]\n",
            " [-8.860666 ]]\n",
            "5000 0.036090225 [[ 7.3946624]\n",
            " [ 1.3073424]\n",
            " [ 0.6075323]\n",
            " [-3.7474756]\n",
            " [-9.168227 ]]\n",
            "5500 0.029718116 [[ 7.6950502 ]\n",
            " [ 1.4297767 ]\n",
            " [ 0.61094975]\n",
            " [-3.8787024 ]\n",
            " [-9.424666  ]]\n",
            "6000 0.025099564 [[ 7.949458 ]\n",
            " [ 1.5399293]\n",
            " [ 0.6132085]\n",
            " [-3.9934726]\n",
            " [-9.643544 ]]\n",
            "6500 0.021622866 [[ 8.168949 ]\n",
            " [ 1.6400197]\n",
            " [ 0.6147342]\n",
            " [-4.0953317]\n",
            " [-9.833825 ]]\n",
            "7000 0.018925149 [[  8.361195  ]\n",
            " [  1.73173   ]\n",
            " [  0.61577797]\n",
            " [ -4.1868076 ]\n",
            " [-10.0016985 ]]\n",
            "7500 0.016779087 [[  8.531695 ]\n",
            " [  1.8163563]\n",
            " [  0.6164977]\n",
            " [ -4.269752 ]\n",
            " [-10.1516   ]]\n",
            "8000 0.015036499 [[  8.684508 ]\n",
            " [  1.8949164]\n",
            " [  0.6169938]\n",
            " [ -4.3455734]\n",
            " [-10.28679  ]]\n",
            "8500 0.013596927 [[  8.822688  ]\n",
            " [  1.9682243 ]\n",
            " [  0.61733377]\n",
            " [ -4.415359  ]\n",
            " [-10.40975   ]]\n",
            "9000 0.012389941 [[  8.948595 ]\n",
            " [  2.0369387]\n",
            " [  0.6175633]\n",
            " [ -4.479966 ]\n",
            " [-10.522382 ]]\n",
            "9500 0.011365108 [[  9.064079 ]\n",
            " [  2.1016006]\n",
            " [  0.6177139]\n",
            " [ -4.5400887]\n",
            " [-10.62619  ]]\n",
            "10000 0.010485378 [[  9.170615 ]\n",
            " [  2.1626625]\n",
            " [  0.6178107]\n",
            " [ -4.596286 ]\n",
            " [-10.72239  ]]\n",
            "\n",
            "Hypothesis: \n",
            " [[0.00840268]\n",
            " [0.99378586]\n",
            " [0.9864831 ]\n",
            " [0.01356781]] \n",
            "Correct:\n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:\n",
            " 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feTXLhCLxpbG",
        "outputId": "b35657d0-e128-4eef-e0ed-dc3384f59920"
      },
      "source": [
        "#위의 코드에서 히든 레이어를 3개로 늘리기(노드는 5개)\n",
        "#정밀도가 늘어나지는 않음\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.1\n",
        "tf.set_random_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "x_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_data = [[0], [1], [1], [0]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "#레이어 시작\n",
        "W1 = tf.Variable(tf.random_normal([2, 5]), name='weight1')\n",
        "b1 = tf.Variable(tf.random_normal([5]), name='bias1')\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "#===========================추가코드========================\n",
        "W2 = tf.Variable(tf.random_normal([5, 5]), name='weight2')\n",
        "b2 = tf.Variable(tf.random_normal([5]), name='bias2')\n",
        "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([5, 5]), name='weight3')\n",
        "b3 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
        "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
        "#============================================================\n",
        "W4 = tf.Variable(tf.random_normal([5, 1]), name='weight4')\n",
        "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
        "#여기까지가 그래프를 그리는 레이어다\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
        "        if step % 500 == 0:\n",
        "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W2))\n",
        "            \n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.69570804 [[-1.2672719  -1.4244449   0.4453939   2.255021    1.3681967 ]\n",
            " [ 2.040617    1.2754596  -0.28644198  0.36494327 -1.2758965 ]\n",
            " [ 1.7917694   0.92977893  1.7656289   0.5455362  -0.36813128]\n",
            " [ 0.08228423  0.30296504  0.65930617  0.46669954  0.48469266]\n",
            " [-0.16923563 -0.86291474 -0.3715926   0.5440967  -1.6672229 ]]\n",
            "500 0.6930888 [[-1.280101   -1.4297738   0.44497058  2.258312    1.3743409 ]\n",
            " [ 2.0355098   1.275094   -0.28590238  0.3220848  -1.2742167 ]\n",
            " [ 1.7905244   0.92410994  1.765182    0.5371784  -0.36494166]\n",
            " [ 0.09233348  0.299146    0.6585174   0.47132742  0.48391473]\n",
            " [-0.16588618 -0.87027    -0.37134147  0.5465289  -1.6647562 ]]\n",
            "1000 0.69256127 [[-1.2975112  -1.443355    0.44433823  2.2649803   1.3851122 ]\n",
            " [ 2.029908    1.2745821  -0.28466853  0.2818512  -1.2718648 ]\n",
            " [ 1.7878783   0.9172258   1.7653432   0.52709615 -0.36029398]\n",
            " [ 0.10313032  0.29888716  0.658343    0.47127607  0.48206067]\n",
            " [-0.16397303 -0.87714696 -0.37023854  0.5445488  -1.6612607 ]]\n",
            "1500 0.69164234 [[-1.3239446  -1.4697337   0.44338927  2.274432    1.4038749 ]\n",
            " [ 2.023847    1.2764326  -0.2823415   0.24140352 -1.269237  ]\n",
            " [ 1.7829016   0.90990573  1.7663057   0.51279527 -0.35352802]\n",
            " [ 0.11525103  0.30360523  0.65891516  0.4653979   0.4788845 ]\n",
            " [-0.16480814 -0.8832991  -0.36819652  0.5359878  -1.655663  ]]\n",
            "2000 0.6898649 [[-1.367725   -1.5155866   0.44221723  2.2863958   1.4367907 ]\n",
            " [ 2.0188196   1.2872189  -0.277907    0.19769163 -1.2679553 ]\n",
            " [ 1.775493    0.90644324  1.769045    0.4916192  -0.34453437]\n",
            " [ 0.1301009   0.31660983  0.6608449   0.4524831   0.47378364]\n",
            " [-0.17013589 -0.88644713 -0.36441922  0.5187623  -1.6463636 ]]\n",
            "2500 0.68603957 [[-1.4438626  -1.5900879   0.4404212   2.3004508   1.4952037 ]\n",
            " [ 2.0210404   1.3206912  -0.2693112   0.14502174 -1.272294  ]\n",
            " [ 1.7708125   0.92002255  1.7758958   0.4581632  -0.33573622]\n",
            " [ 0.1516557   0.34489468  0.6656565   0.42970267  0.465611  ]\n",
            " [-0.18059911 -0.8797333  -0.35707325  0.48853534 -1.630828  ]]\n",
            "3000 0.675986 [[-1.5763128  -1.7215997   0.43243092  2.318929    1.6061102 ]\n",
            " [ 2.0495734   1.3979572  -0.25148094  0.06809904 -1.2904395 ]\n",
            " [ 1.7942399   0.9744233   1.7918142   0.39733714 -0.3349623 ]\n",
            " [ 0.19201104  0.39665395  0.67646635  0.3883127   0.45421758]\n",
            " [-0.18784395 -0.86018884 -0.3437773   0.43338874 -1.6004405 ]]\n",
            "3500 0.64134866 [[-1.8030144  -2.004504    0.39212173  2.360874    1.850233  ]\n",
            " [ 2.1500072   1.5539343  -0.20882124 -0.09015148 -1.3418847 ]\n",
            " [ 1.9127958   1.1074216   1.8298409   0.2508577  -0.36438432]\n",
            " [ 0.27938986  0.47195297  0.6981318   0.29467258  0.4443457 ]\n",
            " [-0.16117142 -0.8535348  -0.32543105  0.3098621  -1.5243498 ]]\n",
            "4000 0.53312236 [[-2.1442697  -2.493029    0.2582223   2.5564775   2.2797942 ]\n",
            " [ 2.361555    1.7900797  -0.11230523 -0.4810061  -1.4761691 ]\n",
            " [ 2.1726284   1.3208765   1.9150519  -0.12484036 -0.48809442]\n",
            " [ 0.42728797  0.5422074   0.7324286   0.0854833   0.4179368 ]\n",
            " [-0.07459769 -0.9069288  -0.31736028  0.06204519 -1.4204937 ]]\n",
            "4500 0.34651744 [[-2.3833578  -2.9644072   0.02129656  3.1707332   2.5961168 ]\n",
            " [ 2.6036608   1.9571681   0.01103148 -0.9481585  -1.6515718 ]\n",
            " [ 2.4780102   1.4615468   2.0235164  -0.5297629  -0.6737991 ]\n",
            " [ 0.6129619   0.55089855  0.75962234 -0.04993768  0.34240976]\n",
            " [ 0.09333804 -1.0467144  -0.3536161   0.04635838 -1.419336  ]]\n",
            "5000 0.17585185 [[-2.572704   -3.271102   -0.19496423  3.7447915   2.832244  ]\n",
            " [ 2.7686837   2.1069424   0.12666143 -1.3053898  -1.7821803 ]\n",
            " [ 2.701772    1.6120665   2.1363595  -0.8364733  -0.82342964]\n",
            " [ 0.7506375   0.600803    0.79757106 -0.1363123   0.27474546]\n",
            " [ 0.182645   -1.0850865  -0.37491232  0.09062327 -1.4123554 ]]\n",
            "5500 0.108847745 [[-2.708027   -3.4038558  -0.3101579   4.033166    2.9673495 ]\n",
            " [ 2.8621886   2.1913214   0.19818196 -1.4737904  -1.8648247 ]\n",
            " [ 2.8266973   1.6958336   2.207305   -0.97586715 -0.9162188 ]\n",
            " [ 0.8209901   0.6351862   0.8261831  -0.1752304   0.23195224]\n",
            " [ 0.20872042 -1.0856148  -0.3779954   0.12317809 -1.4112314 ]]\n",
            "6000 0.07741088 [[-2.795954   -3.479067   -0.3787855   4.2028465   3.0479608 ]\n",
            " [ 2.921605    2.2396395   0.24458276 -1.5699601  -1.9189264 ]\n",
            " [ 2.9052186   1.7422915   2.2533052  -1.0527126  -0.97619677]\n",
            " [ 0.8636758   0.6542245   0.8459728  -0.19724491  0.20386444]\n",
            " [ 0.22229333 -1.0868028  -0.37685156  0.14516698 -1.4139484 ]]\n",
            "6500 0.059677534 [[-2.858062   -3.5295172  -0.42539066  4.318117    3.1021512 ]\n",
            " [ 2.9633672   2.2710147   0.2775638  -1.6339647  -1.957481  ]\n",
            " [ 2.9602442   1.7719976   2.2861001  -1.1027454  -1.0189134 ]\n",
            " [ 0.893039    0.66630685  0.8605449  -0.21211496  0.18361819]\n",
            " [ 0.23110789 -1.0887429  -0.37484345  0.16100049 -1.4175812 ]]\n",
            "7000 0.048406556 [[-2.9050894  -3.5666642  -0.45973617  4.4036365   3.1416996 ]\n",
            " [ 2.9948459   2.2933326   0.30267382 -1.6807367  -1.9868712 ]\n",
            " [ 3.001692    1.7929515   2.3111596  -1.138762   -1.0515593 ]\n",
            " [ 0.91487604  0.67479503  0.871908   -0.22320211  0.1679986 ]\n",
            " [ 0.23739204 -1.0908653  -0.3726748   0.17310126 -1.4213299 ]]\n",
            "7500 0.04064604 [[-2.9424915  -3.5956764  -0.48644346  4.4708247   3.172214  ]\n",
            " [ 3.0197513   2.3102243   0.32274088 -1.7170657  -2.0103717 ]\n",
            " [ 3.034488    1.8087323   2.3312564  -1.1664301  -1.0777519 ]\n",
            " [ 0.93198526  0.6811766   0.8811522  -0.23199208  0.15537226]\n",
            " [ 0.24212366 -1.0929351  -0.37052742  0.18276158 -1.4249663 ]]\n",
            "8000 0.034990974 [[-2.973309   -3.6192749  -0.5080081   4.5257525   3.1967087 ]\n",
            " [ 3.040156    2.3235843   0.33934873 -1.7465035  -2.0298243 ]\n",
            " [ 3.06137     1.8211718   2.3479445  -1.1886601  -1.099508  ]\n",
            " [ 0.9458964   0.6862051   0.8889119  -0.23925619  0.14481962]\n",
            " [ 0.24582133 -1.0949036  -0.36845207  0.19072434 -1.4284172 ]]\n",
            "8500 0.030693322 [[-2.999381   -3.6390471  -0.52591175  4.571979    3.216957  ]\n",
            " [ 3.0573175   2.3344986   0.35345745 -1.7711053  -2.0463464 ]\n",
            " [ 3.083992    1.8313106   2.362163   -1.2071134  -1.1180482 ]\n",
            " [ 0.9575227   0.6903051   0.895581   -0.24544011  0.13577947]\n",
            " [ 0.24879077 -1.0967501  -0.36646235  0.19744873 -1.4316734 ]]\n",
            "9000 0.027319897 [[-3.021892   -3.6559923  -0.5410967   4.611747    3.2340763 ]\n",
            " [ 3.072048    2.3436384   0.36568633 -1.7921522  -2.0606592 ]\n",
            " [ 3.1034222   1.8397864   2.374522   -1.2228137  -1.1341625 ]\n",
            " [ 0.967449    0.6937355   0.901419   -0.25082216  0.12788725]\n",
            " [ 0.25122598 -1.0984787  -0.36455816  0.20323563 -1.4347422 ]]\n",
            "9500 0.024603317 [[-3.0416443  -3.670772   -0.5541955   4.6465497   3.2488117 ]\n",
            " [ 3.0848978   2.3514402   0.37645566 -1.8104887  -2.0732563 ]\n",
            " [ 3.120384    1.8470141   2.3854346  -1.2364311  -1.1483871 ]\n",
            " [ 0.9760683   0.69666356  0.9066044  -0.25558683  0.12089363]\n",
            " [ 0.25325662 -1.1001011  -0.36273503  0.20829146 -1.4376366 ]]\n",
            "10000 0.02236979 [[-3.0592043  -3.6838493  -0.5656514   4.677434    3.26168   ]\n",
            " [ 3.0962563   2.3582056   0.3860621  -1.8266995  -2.0844812 ]\n",
            " [ 3.1353862   1.8532753   2.3951924  -1.2484242  -1.1611016 ]\n",
            " [ 0.9836565   0.6992033   0.9112645  -0.25986242  0.11462137]\n",
            " [ 0.25497317 -1.1016232  -0.36098808  0.2127631  -1.4403722 ]]\n",
            "\n",
            "Hypothesis: \n",
            " [[0.02426261]\n",
            " [0.97993314]\n",
            " [0.98105216]\n",
            " [0.02519399]] \n",
            "Correct:\n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:\n",
            " 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j3nSDBtex0eb",
        "outputId": "7dbeb883-f6dc-4ad1-be62-4260ec91d042"
      },
      "source": [
        "#레이어 4개일 때 cost와 accuracy 비교\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "epoch_arr = []\n",
        "cost_arr = []\n",
        "accuracy_arr = []\n",
        "step_val = 10000\n",
        "\n",
        "def graph():\n",
        "    import matplotlib as mpl\n",
        "    mpl.rc('axes', unicode_minus=False)    #음수부호 설정\n",
        "    fig, ax0 = plt.subplots()\n",
        "    ax1 = ax0.twinx()    #x축을 공유하는 두개의 y축\n",
        "    ax0.set_title(\"Epoch : cost / accuracy\")\n",
        "    ax0.plot(cost_arr, 'r-', label='cost')\n",
        "    ax0.set_ylabel('cost')\n",
        "    ax0.axis([0, step_val, 0, 1])\n",
        "    ax0.grid(True)\n",
        "    \n",
        "    ax1.plot(accuracy_arr, 'b', label='accuracy')\n",
        "    ax1.set_ylabel('accuracy')\n",
        "    ax1.grid(False)\n",
        "    ax1.set_xlabel('epochs')\n",
        "    ax1.axis([0, step_val, 0, 1])\n",
        "    plt.show()\n",
        "    \n",
        "learning_rate = 0.1\n",
        "tf.set_random_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "x_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_data = [[0], [1], [1], [0]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "#레이어 시작\n",
        "W1 = tf.Variable(tf.random_normal([2, 5]), name='weight1')\n",
        "b1 = tf.Variable(tf.random_normal([5]), name='bias1')\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([5, 5]), name='weight2')\n",
        "b2 = tf.Variable(tf.random_normal([5]), name='bias2')\n",
        "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([5, 5]), name='weight3')\n",
        "b3 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
        "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
        "\n",
        "\n",
        "W4 = tf.Variable(tf.random_normal([5, 5]), name='weight4')\n",
        "b4 = tf.Variable(tf.random_normal([5]), name='bias4')\n",
        "layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
        "\n",
        "#레이어가 많아지면 기울기 소실 문제가 발생하여 오히려 오차가 커진다\n",
        "W5 = tf.Variable(tf.random_normal([5, 5]), name='weight5')\n",
        "b5 = tf.Variable(tf.random_normal([5]), name='bias5')\n",
        "layer5 = tf.sigmoid(tf.matmul(layer4, W5) + b5)\n",
        "\n",
        "W6 = tf.Variable(tf.random_normal([5, 1]), name='weight6')\n",
        "b6 = tf.Variable(tf.random_normal([1]), name='bias7')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
        "#여기까지가 그래프를 그리는 레이어다\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(step_val):\n",
        "        _, h, p, a, c = sess.run([train, hypothesis, predicted, accuracy, cost], feed_dict={X:x_data, Y:y_data})\n",
        "        \n",
        "        epoch_arr.append(step)\n",
        "        cost_arr.append(c)\n",
        "        accuracy_arr.append(a)\n",
        "        \n",
        "        if step % (step_val/10) == 0:\n",
        "            print(\"Epoch:\", step, \"\\nHypothesis:\\n\", h, \"\\nAccuracy:\\n\", a, \"\\n\\n\")\n",
        "        h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "        \n",
        "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)\n",
        "    \n",
        "graph()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \n",
            "Hypothesis:\n",
            " [[0.68079114]\n",
            " [0.6829269 ]\n",
            " [0.68030196]\n",
            " [0.6824511 ]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 1000 \n",
            "Hypothesis:\n",
            " [[0.49911243]\n",
            " [0.5012911 ]\n",
            " [0.4986962 ]\n",
            " [0.50082004]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 2000 \n",
            "Hypothesis:\n",
            " [[0.49912333]\n",
            " [0.50129676]\n",
            " [0.49871334]\n",
            " [0.5007935 ]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 3000 \n",
            "Hypothesis:\n",
            " [[0.49912518]\n",
            " [0.5013166 ]\n",
            " [0.49871755]\n",
            " [0.50077415]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 4000 \n",
            "Hypothesis:\n",
            " [[0.49911785]\n",
            " [0.5013525 ]\n",
            " [0.49870828]\n",
            " [0.5007615 ]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 5000 \n",
            "Hypothesis:\n",
            " [[0.49910057]\n",
            " [0.5014069 ]\n",
            " [0.49868423]\n",
            " [0.5007553 ]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 6000 \n",
            "Hypothesis:\n",
            " [[0.49907172]\n",
            " [0.5014833 ]\n",
            " [0.49864295]\n",
            " [0.50075483]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 7000 \n",
            "Hypothesis:\n",
            " [[0.49902922]\n",
            " [0.50158757]\n",
            " [0.4985812 ]\n",
            " [0.5007602 ]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 8000 \n",
            "Hypothesis:\n",
            " [[0.49897003]\n",
            " [0.5017284 ]\n",
            " [0.49849397]\n",
            " [0.5007715 ]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "Epoch: 9000 \n",
            "Hypothesis:\n",
            " [[0.49888834]\n",
            " [0.5019185 ]\n",
            " [0.4983727 ]\n",
            " [0.50078833]] \n",
            "Accuracy:\n",
            " 0.5 \n",
            "\n",
            "\n",
            "\n",
            "Hypothesis: \n",
            " [[0.49877614]\n",
            " [0.50217986]\n",
            " [0.49820483]\n",
            " [0.5008111 ]] \n",
            "Correct:\n",
            " [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "Accuracy:\n",
            " 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGJ-3HeTyHYG",
        "outputId": "68c5c003-a699-49fb-b879-f3c57af16353"
      },
      "source": [
        "#기울기 소실 문제와 활성화 함수(ReLU)\n",
        "#ReLU함수 : 시그모이드 함수의 대안으로 현재 가장 많이 사용\\\n",
        "#위에선 히든 레이어가 5개이면 결과가 오히려 부정확해졌으나\n",
        "#ReLU를 쓰면 더 정확하게 할 수 있다\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.01\n",
        "tf.set_random_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "x_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "y_data = [[0], [1], [1], [0]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "#레이어 시작\n",
        "W1 = tf.Variable(tf.random_normal([2, 5]), name='weight1')\n",
        "b1 = tf.Variable(tf.zeros([5]), name='bias1')\n",
        "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)  #시그모이드 함수 대신에 ReLU함수 사용\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([5, 5]), name='weight2')\n",
        "b2 = tf.Variable(tf.random_normal([5]), name='bias2')\n",
        "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([5, 5]), name='weight3')\n",
        "b3 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
        "layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
        "\n",
        "W4 = tf.Variable(tf.random_normal([5, 5]), name='weight3')\n",
        "b4 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
        "layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
        "\n",
        "W5 = tf.Variable(tf.random_normal([5, 5]), name='weight3')\n",
        "b5 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
        "layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
        "\n",
        "W6 = tf.Variable(tf.random_normal([5, 1]), name='weight4')\n",
        "b6 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)    #맨 끝에는 relu가 아니라 sigmoid써야 된다\n",
        "#여기까지가 그래프를 그리는 레이어다\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        sess.run(train, feed_dict = {X:x_data, Y:y_data})\n",
        "        if step % 1000 == 0:\n",
        "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
        "            \n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "    print(\"\\nHypothesis: \\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.9906736\n",
            "1000 0.0058720596\n",
            "2000 0.0020654462\n",
            "3000 0.0011754337\n",
            "4000 0.00079587527\n",
            "5000 0.00059150445\n",
            "6000 0.00046679156\n",
            "7000 0.00038547852\n",
            "8000 0.00032861758\n",
            "9000 0.00028597188\n",
            "10000 0.00025270766\n",
            "\n",
            "Hypothesis: \n",
            " [[7.4186921e-04]\n",
            " [9.9996912e-01]\n",
            " [9.9982101e-01]\n",
            " [5.8829417e-05]] \n",
            "Correct:\n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:\n",
            " 1.0\n"
          ]
        }
      ]
    }
  ]
}